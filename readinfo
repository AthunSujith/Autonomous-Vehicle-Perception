project_root/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ kitti/
â”‚   â”‚       â”œâ”€â”€ images/          # unzipped from data_object_image_2.zip
â”‚   â”‚       â”‚   â”œâ”€â”€ 000000.png
â”‚   â”‚       â”‚   â”œâ”€â”€ 000001.png
â”‚   â”‚       â”‚   â””â”€â”€ ...
â”‚   â”‚       â””â”€â”€ labels/          # unzipped from data_object_label_2.zip
â”‚   â”‚           â”œâ”€â”€ 000000.txt
â”‚   â”‚           â”œâ”€â”€ 000001.txt
â”‚   â”‚           â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”‚   â”œâ”€â”€ train/           # training images after split
â”‚   â”‚   â”‚   â”œâ”€â”€ val/             # validation images
â”‚   â”‚   â”‚   â””â”€â”€ test/            # test images
â”‚   â”‚   â””â”€â”€ annotations/
â”‚   â”‚       â”œâ”€â”€ kitti_coco.json          # full json (all images)
â”‚   â”‚       â”œâ”€â”€ kitti_coco_train.json    # split json
â”‚   â”‚       â”œâ”€â”€ kitti_coco_val.json
â”‚   â”‚       â””â”€â”€ kitti_coco_test.json
â”‚   â”‚
â”‚   â””â”€â”€ sample/                  # optional 200-image subset for quick debugging
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ convert_kitti_to_coco.py
â”‚   â”œâ”€â”€ split_coco.py
â”‚   â”œâ”€â”€ train_yolov8.sh
â”‚   â”œâ”€â”€ train_detectron.py
â”‚   â”œâ”€â”€ register_coco_dataset.py
â”‚   â””â”€â”€ inference_demo.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ eval_coco.py
â”‚   â””â”€â”€ visualize.py
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ runs/          # YOLO training outputs
â”‚   â”œâ”€â”€ detectron2_kitti/   # Detectron2 outputs
â”‚   â””â”€â”€ vis/           # visualizations / annotated samples
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ visualize_dataset.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸš— Autonomous Vehicle Perception (Mini Simulation)
1. Project Overview

This project builds a computer vision pipeline that allows a machine (like a self-driving car) to detect objects in its environment (cars, pedestrians, traffic signs, etc.) using deep learning.

We simulate the perception system of a self-driving car by:

Training a YOLO (You Only Look Once) object detection model.

Using a benchmark dataset (KITTI or COCO-style) of real road scenes.

Creating a demo app (Streamlit) that allows inference on images, videos, or live webcam streams.

2. Why This Project is Used

Self-Driving Cars: Object detection is the foundation of autonomous navigation. Without detecting other vehicles, pedestrians, or traffic signs, a car cannot make safe decisions.

AI Career Skills: YOLO and CNN-based detection are industry-standard. Learning them makes you job-ready for roles in computer vision, AI engineering, robotics, and ADAS (Advanced Driver Assistance Systems).

Practical Simulation: Even though we are not controlling a real car, this project simulates the â€œeyesâ€ of a self-driving system. Thatâ€™s why itâ€™s called a mini simulation.

Transferable Knowledge: The same perception system is used in drones, surveillance, robotics, and industrial automation.

3. Key Components
ğŸ”¹ (A) Dataset â€” KITTI Benchmark

What it is: KITTI contains thousands of road scene images with labeled bounding boxes (cars, cyclists, pedestrians, etc.).

Why: This is one of the most widely used datasets for training self-driving perception models, ensuring our project is realistic.

How we use it: We download it, process annotations into YOLO format (images + labels), and feed it into our training pipeline.

ğŸ”¹ (B) Model â€” YOLOv8

What it is: YOLO (You Only Look Once) is a family of real-time object detectors.

Why YOLOv8:

State-of-the-art accuracy and speed.

Works on both CPU and GPU.

Supports multiple export formats (TorchScript, ONNX, TensorRT).

How we use it:

Start with a pretrained YOLO (on COCO dataset).

Fine-tune it on KITTI data.

Save best weights (best.pt).

ğŸ”¹ (C) Training

Steps:

Define data.yaml (paths to train/val images + class names).

Train with yolo task=detect mode=train model=yolov8n.pt data=data/kitti.yaml.

Validate performance (mAP, precision, recall).

Why: Training ensures the model adapts to road-specific objects instead of generic COCO categories.

ğŸ”¹ (D) Deployment & Demo App

We created a Streamlit demo app with three modes:

Image mode â†’ upload an image â†’ see bounding boxes + labels.

Video mode â†’ upload a video â†’ run detection on frames.

Webcam mode â†’ live detection (simulating a dashcam).

Why: This makes the project interactive and easy to showcase (interview-ready).

How: Streamlit provides a simple Python interface for UI, while YOLO runs inference behind the scenes.

4. Project Workflow

Environment setup â†’ requirements.txt, virtualenv, install YOLO.
Why: Ensures reproducibility.

Data prep â†’ download KITTI, unzip, convert annotations to YOLO format.
Why: Models need correctly structured datasets.

Train model â†’ fine-tune YOLOv8.
Why: Transfer learning reduces training time and increases accuracy.

Evaluate model â†’ check precision/recall/mAP.
Why: Verify it generalizes well.

Deploy model â†’ create demo app (image/video/live).
Why: Make it user-friendly and real-world-like.

5. Why Recruiters & Industry Care

âœ… Demonstrates end-to-end AI project skills: dataset handling â†’ training â†’ deployment.

âœ… Relevant to high-demand fields: self-driving, robotics, smart cameras.

âœ… Uses real-world benchmark datasets: KITTI is respected in academia and industry.

âœ… Showcases model deployment ability: Employers want engineers who can deploy, not just train models.

âœ… Portfolio-ready: The demo app is interactive and can be shown in an interview.

6. Extensions / Next Steps

Add semantic segmentation (road lanes, drivable area).

Add tracking (follow objects frame by frame).

Export model to ONNX / TensorRT for edge deployment.

Integrate into a ROS (Robot Operating System) simulation for robotics.

Connect to a simulated driving environment (CARLA simulator).
